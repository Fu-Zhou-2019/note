---
layout: draft
title: "Optimization"
permalink: /convexopt
date: 2020-03-22 14:49:0 +0000
comments: False
share: False
categories: ml
---


[Dimitri P. Bertsekas - Optimization Society Prize](https://www.youtube.com/watch?v=T-fSmSqzcqE)

[https://zhuanlan.zhihu.com/p/47453144](https://zhuanlan.zhihu.com/p/47453144)


## Deep learning Optimizers

[Recent Optimization Algorithms for Deep Learning](https://nicklashansen.github.io/)

**[Adaptive Learning of the Optimal Mini-Batch Size of SGD,Arxiv2005](https://arxiv.org/pdf/2005.01097.pdf)**

Adaptively change the batch size


## ADMM

**[Differentiable Linearized ADMM,ICML19](https://arxiv.org/abs/1905.06179)**

## Dive into nn

**[Gradient Descent Finds Global Minima of Deep Neural Networks,ICML19](https://arxiv.org/pdf/1811.03804.pdf)**

## Deep Declarative Network

[https://anucvml.github.io/ddn-cvprw2020/#invited-speakers](https://anucvml.github.io/ddn-cvprw2020/#invited-speakers)

**[OptNet: Differentiable Optimization as a Layer in Neural Networks,ICML17](https://arxiv.org/pdf/1703.00443.pdf)**

[github](https://github.com/locuslab/optnet)



**[Meta-learning with differentiable closed-form solvers,ICLR19](https://openreview.net/forum?id=HyxnZh0ct7)**

**[Meta-Learning with Differentiable Convex Optimization,CVPR19](http://openaccess.thecvf.com/content_CVPR_2019/papers/Lee_Meta-Learning_With_Differentiable_Convex_Optimization_CVPR_2019_paper.pdf)**

**[Learning End-to-end Video Classification with Rank-Pooling,ICML16](http://proceedings.mlr.press/v48/fernando16.pdf)**

## MISC

**[How Many Samples is a Good Initial Point Worth?](https://arxiv.org/pdf/2006.06915.pdf)**

> Optimizing the threshold over regions of the landscape, we
see that, for initial points not too close to the ground truth, a linear improvement
in the quality of the initial guess amounts to a constant factor improvement in the
sample complexity.


---
layout: draft
title: "Distillation"
date: 2020-03-22 14:49:0 +0000
comments: False
share: False
categories: cv
---

**[Online Knowledge Distillation with Diverse Peers,AAAI20](https://arxiv.org/pdf/1912.00350.pdf)**

[code](https://github.com/DefangChen/OKDDip)



## Self-Training

**[Self-training with Noisy Student improves ImageNet classification,CVPR20](https://arxiv.org/pdf/1911.04252.pdf)**

[reddit](https://www.reddit.com/r/MachineLearning/comments/dvh8e8/191104252_selftraining_with_noisy_student/)


> Our key improvements lie in adding noise to the student
and using student models that are equal to or larger than the
teacher. This makes our method different from Knowledge
Distillation [33] where adding noise is not the core concern
and a small model is often used as a student to be faster than
the teacher. One can think of our method as Knowledge
Expansion in which we want the student to be better than
the teacher by giving the student model enough capacity and
difficult environments in terms of noise to learn through.

**[Rethinking Pre-training and Self-training](https://arxiv.org/pdf/2006.06882.pdf)**

> Our study reveals the generality and
flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike
pre-training, self-training is always helpful when using stronger data augmentation,
in both low-data and high-data regimes, and 3) in the case that pre-training is
helpful, self-training improves upon pre-training.Our study reveals the generality and
flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike
pre-training, self-training is always helpful when using stronger data augmentation,
in both low-data and high-data regimes, and 3) in the case that pre-training is
helpful, self-training improves upon pre-training.








